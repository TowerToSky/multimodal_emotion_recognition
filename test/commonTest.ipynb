{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 介绍\n",
    "\n",
    "一个常用的测试的notebook文件，用于查看想要查看的内容，或者一些测试代码等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导包\n",
    "通用导包，所有导包都放在这儿，之后测试一次性加载即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/nvme1/yihaoyuan/Raven/RavenEx/cross_modality_experiment/multimodal_emotion_recognition\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 获取项目根目录的绝对路径\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "print(project_root)\n",
    "# 将项目根目录添加到 sys.path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from common import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<class 'models.CTFNPromptModels.ModalityAlign'>\n",
      "odict_keys(['extractor.feature_global.gcn1.weight', 'extractor.feature_global.gcn1.bias', 'extractor.feature_global.gcn2.weight', 'extractor.feature_global.gcn2.bias', 'extractor.feature_global.gcn3.weight', 'extractor.feature_global.gcn3.bias', 'extractor.feature_global.pool.attn_gcn.weight', 'extractor.feature_global.pool.attn_gcn.bias', 'align.docking_0.weight', 'align.docking_0.bias', 'align.docking_1.weight', 'align.docking_1.bias', 'align.docking_2.weight', 'align.docking_2.bias', 'align.relative_position.0', 'align.relative_position.1', 'align.relative_position.2'])\n",
      "<class 'models.CTFNPromptModels.DoubleTrans'>\n",
      "<models.CTFNPromptModels.DoubleTrans object at 0x7fb3160e1fd0>\n",
      "<bound method DoubleTrans.return_models of <models.CTFNPromptModels.DoubleTrans object at 0x7fb3160e1fd0>>\n",
      "odict_keys(['relative_pos', 'linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias', 'layers.0.self_attn.in_proj_weight', 'layers.0.self_attn.in_proj_bias', 'layers.0.self_attn.out_proj.weight', 'layers.0.self_attn.out_proj.bias', 'layers.0.linear1.weight', 'layers.0.linear1.bias', 'layers.0.linear2.weight', 'layers.0.linear2.bias', 'layers.0.norm.weight', 'layers.0.norm.bias', 'layers.1.self_attn.in_proj_weight', 'layers.1.self_attn.in_proj_bias', 'layers.1.self_attn.out_proj.weight', 'layers.1.self_attn.out_proj.bias', 'layers.1.linear1.weight', 'layers.1.linear1.bias', 'layers.1.linear2.weight', 'layers.1.linear2.bias', 'layers.1.norm.weight', 'layers.1.norm.bias', 'layers.2.self_attn.in_proj_weight', 'layers.2.self_attn.in_proj_bias', 'layers.2.self_attn.out_proj.weight', 'layers.2.self_attn.out_proj.bias', 'layers.2.linear1.weight', 'layers.2.linear1.bias', 'layers.2.linear2.weight', 'layers.2.linear2.bias', 'layers.2.norm.weight', 'layers.2.norm.bias', 'norm.weight', 'norm.bias'])\n",
      "odict_keys(['relative_pos', 'linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias', 'layers.0.self_attn.in_proj_weight', 'layers.0.self_attn.in_proj_bias', 'layers.0.self_attn.out_proj.weight', 'layers.0.self_attn.out_proj.bias', 'layers.0.linear1.weight', 'layers.0.linear1.bias', 'layers.0.linear2.weight', 'layers.0.linear2.bias', 'layers.0.norm.weight', 'layers.0.norm.bias', 'layers.1.self_attn.in_proj_weight', 'layers.1.self_attn.in_proj_bias', 'layers.1.self_attn.out_proj.weight', 'layers.1.self_attn.out_proj.bias', 'layers.1.linear1.weight', 'layers.1.linear1.bias', 'layers.1.linear2.weight', 'layers.1.linear2.bias', 'layers.1.norm.weight', 'layers.1.norm.bias', 'layers.2.self_attn.in_proj_weight', 'layers.2.self_attn.in_proj_bias', 'layers.2.self_attn.out_proj.weight', 'layers.2.self_attn.out_proj.bias', 'layers.2.linear1.weight', 'layers.2.linear1.bias', 'layers.2.linear2.weight', 'layers.2.linear2.bias', 'layers.2.norm.weight', 'layers.2.norm.bias', 'norm.weight', 'norm.bias'])\n",
      "<class 'models.CTFNPromptModels.DoubleTrans'>\n",
      "<models.CTFNPromptModels.DoubleTrans object at 0x7fb31403ac40>\n",
      "<bound method DoubleTrans.return_models of <models.CTFNPromptModels.DoubleTrans object at 0x7fb31403ac40>>\n",
      "odict_keys(['relative_pos', 'linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias', 'layers.0.self_attn.in_proj_weight', 'layers.0.self_attn.in_proj_bias', 'layers.0.self_attn.out_proj.weight', 'layers.0.self_attn.out_proj.bias', 'layers.0.linear1.weight', 'layers.0.linear1.bias', 'layers.0.linear2.weight', 'layers.0.linear2.bias', 'layers.0.norm.weight', 'layers.0.norm.bias', 'layers.1.self_attn.in_proj_weight', 'layers.1.self_attn.in_proj_bias', 'layers.1.self_attn.out_proj.weight', 'layers.1.self_attn.out_proj.bias', 'layers.1.linear1.weight', 'layers.1.linear1.bias', 'layers.1.linear2.weight', 'layers.1.linear2.bias', 'layers.1.norm.weight', 'layers.1.norm.bias', 'layers.2.self_attn.in_proj_weight', 'layers.2.self_attn.in_proj_bias', 'layers.2.self_attn.out_proj.weight', 'layers.2.self_attn.out_proj.bias', 'layers.2.linear1.weight', 'layers.2.linear1.bias', 'layers.2.linear2.weight', 'layers.2.linear2.bias', 'layers.2.norm.weight', 'layers.2.norm.bias', 'norm.weight', 'norm.bias'])\n",
      "odict_keys(['relative_pos', 'linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias', 'layers.0.self_attn.in_proj_weight', 'layers.0.self_attn.in_proj_bias', 'layers.0.self_attn.out_proj.weight', 'layers.0.self_attn.out_proj.bias', 'layers.0.linear1.weight', 'layers.0.linear1.bias', 'layers.0.linear2.weight', 'layers.0.linear2.bias', 'layers.0.norm.weight', 'layers.0.norm.bias', 'layers.1.self_attn.in_proj_weight', 'layers.1.self_attn.in_proj_bias', 'layers.1.self_attn.out_proj.weight', 'layers.1.self_attn.out_proj.bias', 'layers.1.linear1.weight', 'layers.1.linear1.bias', 'layers.1.linear2.weight', 'layers.1.linear2.bias', 'layers.1.norm.weight', 'layers.1.norm.bias', 'layers.2.self_attn.in_proj_weight', 'layers.2.self_attn.in_proj_bias', 'layers.2.self_attn.out_proj.weight', 'layers.2.self_attn.out_proj.bias', 'layers.2.linear1.weight', 'layers.2.linear1.bias', 'layers.2.linear2.weight', 'layers.2.linear2.bias', 'layers.2.norm.weight', 'layers.2.norm.bias', 'norm.weight', 'norm.bias'])\n",
      "<class 'models.CTFNPromptModels.DoubleTrans'>\n",
      "<models.CTFNPromptModels.DoubleTrans object at 0x7fb30476ea90>\n",
      "<bound method DoubleTrans.return_models of <models.CTFNPromptModels.DoubleTrans object at 0x7fb30476ea90>>\n",
      "odict_keys(['relative_pos', 'linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias', 'layers.0.self_attn.in_proj_weight', 'layers.0.self_attn.in_proj_bias', 'layers.0.self_attn.out_proj.weight', 'layers.0.self_attn.out_proj.bias', 'layers.0.linear1.weight', 'layers.0.linear1.bias', 'layers.0.linear2.weight', 'layers.0.linear2.bias', 'layers.0.norm.weight', 'layers.0.norm.bias', 'layers.1.self_attn.in_proj_weight', 'layers.1.self_attn.in_proj_bias', 'layers.1.self_attn.out_proj.weight', 'layers.1.self_attn.out_proj.bias', 'layers.1.linear1.weight', 'layers.1.linear1.bias', 'layers.1.linear2.weight', 'layers.1.linear2.bias', 'layers.1.norm.weight', 'layers.1.norm.bias', 'layers.2.self_attn.in_proj_weight', 'layers.2.self_attn.in_proj_bias', 'layers.2.self_attn.out_proj.weight', 'layers.2.self_attn.out_proj.bias', 'layers.2.linear1.weight', 'layers.2.linear1.bias', 'layers.2.linear2.weight', 'layers.2.linear2.bias', 'layers.2.norm.weight', 'layers.2.norm.bias', 'norm.weight', 'norm.bias'])\n",
      "odict_keys(['relative_pos', 'linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias', 'layers.0.self_attn.in_proj_weight', 'layers.0.self_attn.in_proj_bias', 'layers.0.self_attn.out_proj.weight', 'layers.0.self_attn.out_proj.bias', 'layers.0.linear1.weight', 'layers.0.linear1.bias', 'layers.0.linear2.weight', 'layers.0.linear2.bias', 'layers.0.norm.weight', 'layers.0.norm.bias', 'layers.1.self_attn.in_proj_weight', 'layers.1.self_attn.in_proj_bias', 'layers.1.self_attn.out_proj.weight', 'layers.1.self_attn.out_proj.bias', 'layers.1.linear1.weight', 'layers.1.linear1.bias', 'layers.1.linear2.weight', 'layers.1.linear2.bias', 'layers.1.norm.weight', 'layers.1.norm.bias', 'layers.2.self_attn.in_proj_weight', 'layers.2.self_attn.in_proj_bias', 'layers.2.self_attn.out_proj.weight', 'layers.2.self_attn.out_proj.bias', 'layers.2.linear1.weight', 'layers.2.linear1.bias', 'layers.2.linear2.weight', 'layers.2.linear2.bias', 'layers.2.norm.weight', 'layers.2.norm.bias', 'norm.weight', 'norm.bias'])\n",
      "<class 'models.CTFNPromptModels.EmotionClassificationModel'>\n",
      "odict_keys(['relative_pos', 'encoder.encoder_layer.0.norm_1.alpha', 'encoder.encoder_layer.0.norm_1.bias', 'encoder.encoder_layer.0.norm_2.alpha', 'encoder.encoder_layer.0.norm_2.bias', 'encoder.encoder_layer.0.attn.q.weight', 'encoder.encoder_layer.0.attn.k.weight', 'encoder.encoder_layer.0.attn.v.weight', 'encoder.encoder_layer.0.attn.up.weight', 'encoder.encoder_layer.0.ff.linear_1.weight', 'encoder.encoder_layer.0.ff.linear_1.bias', 'encoder.encoder_layer.0.ff.linear_2.weight', 'encoder.encoder_layer.0.ff.linear_2.bias', 'encoder.encoder_layer.1.norm_1.alpha', 'encoder.encoder_layer.1.norm_1.bias', 'encoder.encoder_layer.1.norm_2.alpha', 'encoder.encoder_layer.1.norm_2.bias', 'encoder.encoder_layer.1.attn.q.weight', 'encoder.encoder_layer.1.attn.k.weight', 'encoder.encoder_layer.1.attn.v.weight', 'encoder.encoder_layer.1.attn.up.weight', 'encoder.encoder_layer.1.ff.linear_1.weight', 'encoder.encoder_layer.1.ff.linear_1.bias', 'encoder.encoder_layer.1.ff.linear_2.weight', 'encoder.encoder_layer.1.ff.linear_2.bias', 'encoder.encoder_layer.2.norm_1.alpha', 'encoder.encoder_layer.2.norm_1.bias', 'encoder.encoder_layer.2.norm_2.alpha', 'encoder.encoder_layer.2.norm_2.bias', 'encoder.encoder_layer.2.attn.q.weight', 'encoder.encoder_layer.2.attn.k.weight', 'encoder.encoder_layer.2.attn.v.weight', 'encoder.encoder_layer.2.attn.up.weight', 'encoder.encoder_layer.2.ff.linear_1.weight', 'encoder.encoder_layer.2.ff.linear_1.bias', 'encoder.encoder_layer.2.ff.linear_2.weight', 'encoder.encoder_layer.2.ff.linear_2.bias', 'encoder.encoder_layer.3.norm_1.alpha', 'encoder.encoder_layer.3.norm_1.bias', 'encoder.encoder_layer.3.norm_2.alpha', 'encoder.encoder_layer.3.norm_2.bias', 'encoder.encoder_layer.3.attn.q.weight', 'encoder.encoder_layer.3.attn.k.weight', 'encoder.encoder_layer.3.attn.v.weight', 'encoder.encoder_layer.3.attn.up.weight', 'encoder.encoder_layer.3.ff.linear_1.weight', 'encoder.encoder_layer.3.ff.linear_1.bias', 'encoder.encoder_layer.3.ff.linear_2.weight', 'encoder.encoder_layer.3.ff.linear_2.bias', 'encoder.encoder_layer.4.norm_1.alpha', 'encoder.encoder_layer.4.norm_1.bias', 'encoder.encoder_layer.4.norm_2.alpha', 'encoder.encoder_layer.4.norm_2.bias', 'encoder.encoder_layer.4.attn.q.weight', 'encoder.encoder_layer.4.attn.k.weight', 'encoder.encoder_layer.4.attn.v.weight', 'encoder.encoder_layer.4.attn.up.weight', 'encoder.encoder_layer.4.ff.linear_1.weight', 'encoder.encoder_layer.4.ff.linear_1.bias', 'encoder.encoder_layer.4.ff.linear_2.weight', 'encoder.encoder_layer.4.ff.linear_2.bias', 'encoder.encoder_layer.5.norm_1.alpha', 'encoder.encoder_layer.5.norm_1.bias', 'encoder.encoder_layer.5.norm_2.alpha', 'encoder.encoder_layer.5.norm_2.bias', 'encoder.encoder_layer.5.attn.q.weight', 'encoder.encoder_layer.5.attn.k.weight', 'encoder.encoder_layer.5.attn.v.weight', 'encoder.encoder_layer.5.attn.up.weight', 'encoder.encoder_layer.5.ff.linear_1.weight', 'encoder.encoder_layer.5.ff.linear_1.bias', 'encoder.encoder_layer.5.ff.linear_2.weight', 'encoder.encoder_layer.5.ff.linear_2.bias', 'encoder.fn.0.weight', 'encoder.fn.0.bias', 'proj1.weight', 'proj1.bias', 'proj2.weight', 'proj2.bias', 'out.weight', 'out.bias'])\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/mnt/nvme1/yihaoyuan/Raven/RavenEx/cross_modality_experiment/multimodal_emotion_recognition/logs/independent/HCI/2024-12-22_20-26-33/best_checkpoint/best_checkpoint_0.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "print(len(checkpoint))\n",
    "# print(checkpoint[0])\n",
    "# print(checkpoint[1].keys())\n",
    "# print(type(checkpoint[0]))\n",
    "# print(checkpoint[0].state_dict().keys())\n",
    "# print(checkpoint[1].state_dict().keys())\n",
    "\n",
    "for i in range(len(checkpoint)):\n",
    "    print(type(checkpoint[i]))\n",
    "    if isinstance(checkpoint[i], nn.Module):\n",
    "        print(checkpoint[i].state_dict().keys())\n",
    "    else:\n",
    "        print(checkpoint[i])\n",
    "        print(checkpoint[i].return_models)\n",
    "        for model in checkpoint[i].return_models():\n",
    "            print(model.state_dict().keys())\n",
    "            print(model.state_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModalityAlign(\n",
      "  (extractor): FeatureExtract(\n",
      "    (feature_global): FeatureGlobal(\n",
      "      (gcn1): GraphConvolution (75 -> 160)\n",
      "      (gcn2): GraphConvolution (160 -> 160)\n",
      "      (gcn3): GraphConvolution (160 -> 160)\n",
      "      (pool): SelfAttentionPooling(\n",
      "        (attn_gcn): GraphConvolution (480 -> 1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (align): Embrace(\n",
      "    (docking_0): Linear(in_features=960, out_features=160, bias=True)\n",
      "    (docking_1): Linear(in_features=41, out_features=160, bias=True)\n",
      "    (docking_2): Linear(in_features=119, out_features=160, bias=True)\n",
      "    (relative_position): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 1x1x160 (GPU 0)]\n",
      "        (1): Parameter containing: [torch.float32 of size 1x1x160 (GPU 0)]\n",
      "        (2): Parameter containing: [torch.float32 of size 1x1x160 (GPU 0)]\n",
      "    )\n",
      "  )\n",
      "), EmotionClassificationModel(\n",
      "  (encoder): AttentionEncoder(\n",
      "    (encoder_layer): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (norm_1): Norm()\n",
      "        (norm_2): Norm()\n",
      "        (attn): Attention(\n",
      "          (q): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (k): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (v): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (up): Conv1d(16, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (drop): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear_1): Linear(in_features=160, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (linear_2): Linear(in_features=2048, out_features=160, bias=True)\n",
      "        )\n",
      "        (dropout_1): DropPath()\n",
      "        (dropout_2): DropPath()\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (norm_1): Norm()\n",
      "        (norm_2): Norm()\n",
      "        (attn): Attention(\n",
      "          (q): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (k): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (v): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (up): Conv1d(16, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (drop): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear_1): Linear(in_features=160, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (linear_2): Linear(in_features=2048, out_features=160, bias=True)\n",
      "        )\n",
      "        (dropout_1): DropPath()\n",
      "        (dropout_2): DropPath()\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (norm_1): Norm()\n",
      "        (norm_2): Norm()\n",
      "        (attn): Attention(\n",
      "          (q): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (k): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (v): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (up): Conv1d(16, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (drop): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear_1): Linear(in_features=160, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (linear_2): Linear(in_features=2048, out_features=160, bias=True)\n",
      "        )\n",
      "        (dropout_1): DropPath()\n",
      "        (dropout_2): DropPath()\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (norm_1): Norm()\n",
      "        (norm_2): Norm()\n",
      "        (attn): Attention(\n",
      "          (q): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (k): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (v): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (up): Conv1d(16, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (drop): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear_1): Linear(in_features=160, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (linear_2): Linear(in_features=2048, out_features=160, bias=True)\n",
      "        )\n",
      "        (dropout_1): DropPath()\n",
      "        (dropout_2): DropPath()\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (norm_1): Norm()\n",
      "        (norm_2): Norm()\n",
      "        (attn): Attention(\n",
      "          (q): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (k): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (v): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (up): Conv1d(16, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (drop): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear_1): Linear(in_features=160, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (linear_2): Linear(in_features=2048, out_features=160, bias=True)\n",
      "        )\n",
      "        (dropout_1): DropPath()\n",
      "        (dropout_2): DropPath()\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (norm_1): Norm()\n",
      "        (norm_2): Norm()\n",
      "        (attn): Attention(\n",
      "          (q): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (k): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (v): Conv1d(160, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (up): Conv1d(16, 160, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (drop): Dropout(p=0.3, inplace=False)\n",
      "        )\n",
      "        (ff): FeedForward(\n",
      "          (linear_1): Linear(in_features=160, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (linear_2): Linear(in_features=2048, out_features=160, bias=True)\n",
      "        )\n",
      "        (dropout_1): DropPath()\n",
      "        (dropout_2): DropPath()\n",
      "      )\n",
      "    )\n",
      "    (fn): Sequential(\n",
      "      (0): Linear(in_features=160, out_features=160, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (proj1): Linear(in_features=160, out_features=160, bias=True)\n",
      "  (proj2): Linear(in_features=160, out_features=160, bias=True)\n",
      "  (out): Linear(in_features=160, out_features=4, bias=True)\n",
      "  (avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True), Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True), Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True), Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True), Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True), Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True), Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True), Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True), Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True), Parameter containing:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "test = [nn.Parameter(torch.zeros(1,10,160)) for _ in range(10)]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50592, 150)\n"
     ]
    }
   ],
   "source": [
    "# 之前的node features形状\n",
    "eeg_path = \"/data/Ruiwen/ExtractedFeatures2/node_features_combine.npy\"\n",
    "eeg = np.load(eeg_path)\n",
    "print(eeg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label', 'subject_list', 'ch_info', 'info', 'eye_info', 'eeg', 'eye', 'au'])\n",
      "EEG: 1-70filer, 50Hz notch, With ICA, 256Hz resample;\n",
      "Subject : 1-34 subject, no 1,23,32 subject, 15 subject exists eye data missing; 31 person, 48 question, 31 channel;\n",
      "Labels : 0: Confused,1: Guess, 2:Unconfused, 4: Think-right;\n",
      "Eye Track data : Pupil diameter left, Pupil diameter right,Gaze point X, Gaze point Y, Eye movement type, Gaze event duration\n",
      "['Fp1', 'F3', 'F7', 'FT9', 'FC5', 'FC1', 'C3', 'T7', 'TP9', 'CP5', 'CP1', 'Pz', 'P3', 'P7', 'O1', 'Oz', 'O2', 'P4', 'P8', 'TP10', 'CP6', 'CP2', 'Cz', 'C4', 'T8', 'FT10', 'FC6', 'FC2', 'F4', 'F8', 'Fp2']\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34]\n"
     ]
    }
   ],
   "source": [
    "# 现有的用来分析的数据形式如下\n",
    "ruiwen_path = \"/data/Ruiwen/data_with_ICA.pkl\"  # 只包含脑电和眼动，不包含人脸\n",
    "ruiwen_data = joblib.load(ruiwen_path)\n",
    "print(ruiwen_data.keys())\n",
    "print(ruiwen_data[\"info\"])\n",
    "print(ruiwen_data['ch_info'])\n",
    "\n",
    "subject_lists = ruiwen_data['subject_list']\n",
    "print(subject_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "eeg_data = ruiwen_data['eeg']\n",
    "print(len(eeg_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "au_feature = ruiwen_data['au']\n",
    "print(len(au_feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:00, 807598.91it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, subject_list in tqdm(enumerate(subject_lists)):\n",
    "    trials = eeg_data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "48\n",
      "(433, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(418, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(596, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(349, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(855, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(389, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(507, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(830, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(617, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(655, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(875, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(854, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(697, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(730, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(673, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(902, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(560, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(848, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(825, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(485, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(875, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(579, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(652, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(581, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(660, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(638, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(442, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(324, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(735, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(904, 6)\n",
      "<class 'list'>\n",
      "48\n",
      "(472, 6)\n"
     ]
    }
   ],
   "source": [
    "# 眼动特征\n",
    "eye_data = ruiwen_data['eye']\n",
    "print(type(eye_data))\n",
    "for per in eye_data:\n",
    "    print(type(per))\n",
    "    print(len(per))\n",
    "    print(per[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Ruiwen\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/data/Ruiwen/data_with_ICA.pkl\"\n",
    "print(utils.find_nearest_folder(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n",
      "(48, 41)\n"
     ]
    }
   ],
   "source": [
    "eye_features_path = \"/data/Ruiwen/eye_track_feature\"\n",
    "for id in subject_lists:\n",
    "    path = os.path.join(eye_features_path, f\"{str(id)}.npy\")\n",
    "    feature = np.load(path)\n",
    "    print(feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 修改数据key，此外把AU添加其中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eeg_data', 'eye_track_data', 'label', 'subject_list', 'ch_info', 'info', 'eye_info'])\n",
      "EEG: 1-70filer, 50Hz notch, With ICA, 256Hz resample;\n",
      "Subject : 1-34 subject, no 1,23,32 subject, 15 subject exists eye data missing; 31 person, 48 question, 31 channel;\n",
      "Labels : 0: Confused,1: Guess, 2:Unconfused, 4: Think-right;\n",
      "Eye Track data : Pupil diameter left, Pupil diameter right,Gaze point X, Gaze point Y, Eye movement type, Gaze event duration\n",
      "['Fp1', 'F3', 'F7', 'FT9', 'FC5', 'FC1', 'C3', 'T7', 'TP9', 'CP5', 'CP1', 'Pz', 'P3', 'P7', 'O1', 'Oz', 'O2', 'P4', 'P8', 'TP10', 'CP6', 'CP2', 'Cz', 'C4', 'T8', 'FT10', 'FC6', 'FC2', 'F4', 'F8', 'Fp2']\n",
      "dict_keys(['label', 'subject_list', 'ch_info', 'info', 'eye_info', 'eeg', 'eye'])\n"
     ]
    }
   ],
   "source": [
    "# 现有的用来分析的数据形式如下\n",
    "ruiwen_path = \"/data/Ruiwen/data_with_ICA.pkl\"  # 只包含脑电和眼动，不包含人脸\n",
    "ruiwen_data = joblib.load(ruiwen_path)\n",
    "print(ruiwen_data.keys())\n",
    "print(ruiwen_data[\"info\"])\n",
    "print(ruiwen_data['ch_info'])\n",
    "# print(ruiwen_data.info)\n",
    "\n",
    "# 首先修改一下key，然后把AU特征也添加其中\n",
    "ruiwen_data['eeg'] = ruiwen_data.pop('eeg_data')\n",
    "ruiwen_data['eye'] = ruiwen_data.pop('eye_track_data')\n",
    "print(ruiwen_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34]\n",
      "31\n",
      "<class 'list'>\n",
      "(31, 48, 119)\n",
      "dict_keys(['label', 'subject_list', 'ch_info', 'info', 'eye_info', 'eeg', 'eye', 'au'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 加载AU特征\n",
    "subject_lists = ruiwen_data['subject_list']\n",
    "print(subject_lists)\n",
    "print(len(subject_lists))\n",
    "print(type(subject_lists))\n",
    "au_path = \"/data/Ruiwen/au_feature\"\n",
    "au_features = []\n",
    "for subject in subject_lists:\n",
    "    au_feature = np.load(os.path.join(au_path, str(subject) + \".npy\"))\n",
    "    # print(au_feature.shape)\n",
    "    au_features.append(au_feature)\n",
    "au_features = np.array(au_features)\n",
    "print(au_features.shape)\n",
    "ruiwen_data['au'] = au_features\n",
    "print(ruiwen_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/Ruiwen/data_with_ICA.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存新的数据\n",
    "ruiwen_path = \"/data/Ruiwen/data_with_ICA.pkl\"\n",
    "joblib.dump(ruiwen_data, ruiwen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "48\n",
      "1847\n",
      "31\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "eeg_data = ruiwen_data[\"eeg_data\"]  # 31，48,（sample_len, ch_nums)\n",
    "print(len(eeg_data))\n",
    "print(len(eeg_data[0]))\n",
    "print(len(eeg_data[0][0]))\n",
    "print(len(eeg_data[0][0][0]))\n",
    "print(len(eeg_data[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractedData\n",
      "adjacency_list_20231217.mat\n",
      "adjacency\n",
      "Raven.zip\n",
      "adjacency_list_9.mat\n",
      "ExtractedFeatures2\n",
      "adjacency_list_8.mat\n",
      "data.pkl\n",
      "zhangyi\n",
      "adjacency_list.mat\n",
      "adjacency_list_20240425.mat\n",
      "adjacency_list_14.mat\n",
      "eye_track_feature\n",
      "adjacency_list_10.mat\n",
      "openface\n",
      "ExtractedFeatures\n",
      "adjacency_list_test.mat\n",
      "class_all.csv\n",
      "Raven\n",
      "dict_data.npy\n",
      "au_feature\n",
      "adjacency_list_multi_all.mat\n",
      "adjacency_list_20231214.mat\n",
      "data_with_ICA.pkl\n",
      "原始数据\n",
      "眼动数据分割.rar\n",
      "adjacency_list_16.mat\n",
      "adjacency_list_15.mat\n",
      "eeg_feature\n"
     ]
    }
   ],
   "source": [
    "# 之前用于训练的数据加载如下\n",
    "data_path = \"/data/Ruiwen\"\n",
    "print(\"\\n\".join(os.listdir(data_path)))\n",
    "eeg_path = os.path.join(data_path, \"ExtractedFeatures2\", \"node_features_combine.npy\")\n",
    "eye_dir_path = os.path.join(data_path, \"eye_track_feature\")\n",
    "face_dir_path = os.path.join(data_path, \"au_feature\")\n",
    "\n",
    "eeg_data = np.load(eeg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试自助法和SMOTE来解决类别不平衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: 0    15\n",
      "1     5\n",
      "dtype: int64\n",
      "\n",
      "Class distribution in resampled bootstrap samples:\n",
      "Bootstrap sample 1 class distribution: 1    16\n",
      "0    16\n",
      "dtype: int64\n",
      "Bootstrap sample 2 class distribution: 0    13\n",
      "1    13\n",
      "dtype: int64\n",
      "Bootstrap sample 3 class distribution: 1    12\n",
      "0    12\n",
      "dtype: int64\n",
      "Bootstrap sample 4 class distribution: 0    15\n",
      "1    15\n",
      "dtype: int64\n",
      "Bootstrap sample 5 class distribution: 1    14\n",
      "0    14\n",
      "dtype: int64\n",
      "\n",
      "Accuracy of model trained on bootstrap sample 1: 0.9\n",
      "\n",
      "Accuracy of model trained on bootstrap sample 2: 0.9\n",
      "\n",
      "Accuracy of model trained on bootstrap sample 3: 0.9\n",
      "\n",
      "Accuracy of model trained on bootstrap sample 4: 1.0\n",
      "\n",
      "Accuracy of model trained on bootstrap sample 5: 0.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# 创建一个不均衡的分类数据集（类别0远多于类别1）\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10],  # 特征\n",
    "              [11], [12], [13], [14], [15], [16], [17], [18], [19], [20]])  \n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # 类别0的样本\n",
    "              1, 1, 1, 0, 0, 0, 0, 0, 1, 1])  # 类别1的样本（类别1非常少）\n",
    "\n",
    "# 查看类别分布\n",
    "print(f\"Original class distribution: {pd.Series(y).value_counts()}\")\n",
    "\n",
    "# 1. 使用自助法生成多个训练集（生成 5 个训练集）\n",
    "num_samples = 5\n",
    "bootstrap_samples = []\n",
    "bootstrap_labels = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    # 抽取自助样本（有放回抽样）\n",
    "    indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "    X_sample, y_sample = X[indices], y[indices]\n",
    "    \n",
    "    # 确保少数类样本数量足够进行SMOTE\n",
    "    minority_class_count = (y_sample == 1).sum()\n",
    "    if minority_class_count < 2:\n",
    "        print(f\"Warning: Sample size for minority class too small in bootstrap sample. Skipping SMOTE for this sample.\")\n",
    "        continue  # 如果少数类样本数过少，跳过该自助样本\n",
    "\n",
    "    # 2. 对每个自助样本使用SMOTE进行少数类过采样\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=3)  # 设置k_neighbors为小于少数类样本数的值\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_sample, y_sample)\n",
    "    \n",
    "    # 存储自助样本和对应的标签\n",
    "    bootstrap_samples.append(X_resampled)\n",
    "    bootstrap_labels.append(y_resampled)\n",
    "\n",
    "# 查看自助样本的类别分布\n",
    "print(f\"\\nClass distribution in resampled bootstrap samples:\")\n",
    "for i in range(num_samples):\n",
    "    print(f\"Bootstrap sample {i+1} class distribution: {pd.Series(bootstrap_labels[i]).value_counts()}\")\n",
    "\n",
    "# 3. 在每个自助样本上训练模型并评估性能\n",
    "for i in range(len(bootstrap_samples)):  # 只在有效的自助样本上训练\n",
    "    X_sample = bootstrap_samples[i]\n",
    "    y_sample = bootstrap_labels[i]\n",
    "    \n",
    "    # 训练决策树分类器\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_sample, y_sample)\n",
    "    \n",
    "    # 预测并计算准确率\n",
    "    y_pred = clf.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    \n",
    "    print(f\"\\nAccuracy of model trained on bootstrap sample {i+1}: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
