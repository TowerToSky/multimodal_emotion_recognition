
# Intro

该文档记录一下自己在编写项目时一些思考

落笔于2024-11-20 16：30



# 2024-11-20 关于从原始数据到手动计算的特征处理

经过简单预处理后的数据存储在'/data/Ruiwen/data_with_ICA.pkl'中，包含'eeg'、'eye'、'au'、'label'、'subject_list'，Info如下：

'''
dict_keys(['label', 'subject_list', 'ch_info', 'info', 'eye_info', 'eeg', 'eye', 'au'])
EEG: 1-70filer, 50Hz notch, With ICA, 256Hz resample;
Subject : 1-34 subject, no 1,23,32 subject, 15 subject exists eye data missing; 31 person, 48 question, 31 channel;
Labels : 0: Confused,1: Guess, 2:Unconfused, 4: Think-right;
Eye Track data : Pupil diameter left, Pupil diameter right,Gaze point X, Gaze point Y, Eye movement type, Gaze event duration
['Fp1', 'F3', 'F7', 'FT9', 'FC5', 'FC1', 'C3', 'T7', 'TP9', 'CP5', 'CP1', 'Pz', 'P3', 'P7', 'O1', 'Oz', 'O2', 'P4', 'P8', 'TP10', 'CP6', 'CP2', 'Cz', 'C4', 'T8', 'FT10', 'FC6', 'FC2', 'F4', 'F8', 'Fp2']
[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34]
'''

> 注意这里没有将AU的信息记录在Info中，后续进行补充。

截至到现在已经编写完加载简单预处理后的原始数据后，对于脑电数据，经过滑窗并计算DE特征，已经用GPT重构代码使其更加工程化。考虑到每次加载时间比较长，所以采用一次加载后就存储保存的形式，后续只需要加载保存好的（一个缺点就是每次保存后的名字是一样的，所以如果其中受试者或频段什么发生改变后，但无法辨识情况下，会加载旧的）（考虑一个解决办法为用名字来唯一标识，后续再说，现在一般也不动）

接下来在写眼动加载的部分代码，由于读取的是原始数据，需要编写眼动数据手动计算特征的代码。

即步骤为：

- 加载数据
- 滑窗
- 手动计算特征，包含41个特征点

后续编写人脸部分的代码同上操作

17：19更新：重复上述步骤有点麻烦，压力有点大，就先按照之前方式，直接加载预先处理好的数据来用

人脸特征，唔，其实存储的原生数据已经是计算后的特征了，而不是原始AU强度点，算了，不管了，直接跟眼动一并处理


17:30更新：已经完成对特征加载代码部分编写，编写于LoadFeatures.py下，接下来将读取到的特征数据，封装成torch.Dataset形式，然后划分训练集和测试集，训练集和测试集划分还是先以留一人交叉验证方式（后续补充跨被试实验，用于丰富文章内容）

# 2024-11-21 20：15，封装成torch.Dataset形式

需要加Norm，但是Norm加载哪儿，需要考虑：

特征上去做Norm，每个特征单独去做Norm
做Zscore-Norm以去除个体差异性，做Min-Max Norm以缩放到-1，1区间内。
如果是在每个人上去做，这样以去除个体差异性

在处理数据时发现一个特点，即在做DE特征提取时，已经在一个通道上一个频带内做了Min-Max归一化，并且对于不满足15s的数据自动补齐数据，用0填充。

如果这时候对其再在每个人内部做Zscore标准化，emmm，会有影响吗？（按之前的训练经验来看是有效的，能够提升准确率。当然也可能是对其他特征做了归一化产生的效果）。TODO

关于脑电数据的补0考虑，因为Ruiwen实验中，每道题目最长时间为15s，（从佳宝CAL得出）

所以最长补充到15s即可

之前脑电形状为（50592，150），即（34\*31\*48，5\*30）

新版本：（34\*48，31，5\*15）

这里的TorchDataset就要做区分训练集和测试集了，那么提前一步，在计算特征时就根据人划分训练和测试，还是说在提取特征中间再加一个模块，用于从提取的特征划分训练集和测试集？用第二个模块更加合适

新模块SplitDataset：用于根据受试者ID获取指定的特征

而FeatureDataset：只需要传入SplitDataset类的数据，就可以将其封装成可供训练和加载的Dataset类

之前的写法是耦合在Dataset中了，因为是留一人交叉验证方式，以及十折交叉验证方式。
（明天再说吧，欸嘿）（划掉）继续干

按Copliate给出的代码，其为个人：特征的字典，算了，还是参考之前的方式去做


考虑一下此时各个特征的形状：

脑电：(1488, 31, 5\*15)
眼动：(1488, 41)
人脸：(1488, 115)
标签：(1488, )


23：41，逻辑通畅了，明天检查一下是否有BUG，然后就可以着手搭建模型了，争取这周修改一版模型，然后训练出一个结果出来。

# 2024-11-25 10：33，检查Dataset逻辑，搭建后续逻辑代码

首先检查撰写的Dataset代码，对于非跨被试和跨被试实验，数据集划分逻辑上是否合理

check done！非跨被试划分无问题，跨被试实验亦无问题。

#TODO：对于2分类任务中，容易出现类别不均衡情况，或者测试集过少情况，后续考虑下

那么当前数据的加载逻辑写完了，下一步是模型搭建，再下一步是训练过程，下一步是测试过程，最后是日志系统，和绘图系统，以及自动化脚本系统。

真的是，一整天，被学术讲座材料给消磨光了，一点都没进度，晚上再rush一把吧

20：30：完成Models部分，以之前写的MAFFM框架（后续再起个名字）

init_weight这种初始化权重方式在开源库中都会存在，说明存在理论支撑在其中

先起名字吧，不然模型不好起名，由AFFM+Transformer结合而出，AFFM名字为Adaptive Feature Fusion Model

多模态注意力特征融合情绪识别策略：Multi-Modal Feature Attention Fusion Encoder strategy 

然后将模型模块化，包含特征提取模型、特征对齐模型、特征融合模型、特征编码模型、分类模型

对于模态融合模块，如果是只有两个模态两两融合那么只有一份参数可以理解

但是对于超过两个模态的情况，如果只用同一个自注意力层进行融合，产生的参数共享，对于不同模态之间注意力信息交叉会产生混淆

解决办法，三种策略：

1）正交顺序融合：EEG融合Eye形成中间态Mid，Mid融合Au；后续添加L1和L2正则化以正交。

2）多个attn：EEG与Eye融合；EEG与Au融合；Eye与Au融合；最后三者拼接。

3）共享的attn：一个attn，其中Eye作为query，与EEG融合；Au作为Q，与EEG融合。

4）现在的最莫名其妙的策略：一个attn，EEG+Eye，EEG+Au，Eye+Au。保留了EEG丰富信息，同时EEG指导

上述融合策略，一个问题即：当EEG完全缺失状态下，真的OK吗？一个想法，Eye、Au作为Query，指导生成EEG中间态表示或融合状态表示，用于后续深度编码（未成熟想法）

之前融合策略中，即上述第4个，EEG作为Q，简而言之，保留了Eye和Au中与EEG相似度最大的特征值，同时保留了两份EEG信息，保留了Eye、Au的最突出的特征信息，保留了Eye特征以及Au中与Eye最相似的特征以及Au最突出特征信息。

这么来看的话，第三步Eye+Au融合意义不大。以及需要加些线性变换来整合信息，之前没有整合。

因此上述策略，需要跑下实验看看效果，预计跑1、3、4，4为ori，但是添加线性变换和移除Eye+Au。

对于EEG完全缺失情况，上述特征，无法得出有效的推导出EEG或受Eye和Au指导的融合信息（唔，加掩码层试试？后续考虑吧，先把上述1、3、4实验跑出来看看效果）

还有在这儿，因为将一整个trial变成了一个特征，那么没有时序信息，那么引入多个Seq以表示不同的子空间表示，拓宽表示空间。

上述4为最终采用的第4版融合策略，后续模态缺失实验是在第一版上实验的。

因此现在还原第4版实验，但是改用EEG+Eye，EEG+Au，同时这边需要再做一组对照实验为Eye+EEG和Au+EEG，即Eye和Au指导EEG（这组实验条件较弱，可以不做）

暂且不考虑单模态情况，只考虑三模态情况，先写的耦合一些

如何避免耦合度来用GCN提取脑电特征是一个问题，TODO

后续再优化吧，先只搭了个骨架。

# 2024-11-26 17:16，跑通模型，完成训练部分搭建

先完成模型搭建部分，以简单快速高效为主来完成，尽快跑起来实验，时间不多了。

配置文件以之前学到的那个自动加载配置文件方式来做，虽然忘了是什么了

21:30：可以打印网络，但是网络输入会存在问题，因为存在需要输入邻接矩阵和图结点，这个在后续编写的时候再做补充

接下来写Trainer部分，用于调用模型进行训练环节。

一个Trainer类往往包含：训练、推理、日志记录三部分

Trainer构建完了，接下来还差Logger、Schedule（可选）、Arguments、Main四个模块，以及shell，哦对，还一个Metrics模块（Acc、F1-score、CM），以及Visualize模块

接下来的任务，构建完Logger模块后，搭建Main逻辑，然后跑通训练

# 2024-11-27 11:13 搭建Main逻辑，跑通训练

哪那么多事，干就完了，多干干总没错，干完一样是一样

昨天的Train.py类可能存在问题，但是问题不大，这个在后续运行Main函数时逐步去看

12：29：main.py的主要逻辑差不多了，后续是在调试中完善，然后config.yaml格式有点问题，不能适配不同的数据集以及不同的模型，需要做点可扩展

然后还有可视化模块的实现，可视化准确率，混淆矩阵，loss和acc曲线在tensorboard中显示了

接下来，下午的任务是打通整个训练链路，跑起来再完善。

22:08：打通训练逻辑，先训练上

eeg：（128，31，75）（这里忘了将后两维度合并了）（哦，这里在后面提取eeg图特征时改变形状了）
eye：（128，41）
au：(128，119)

targets：128

之前脑电为（750，31，150），那么传入到网络中应该为（128，31，150），INPUTS_DIM为（160\*3\*2，41，119）

那么输入的维度为75，即脑电特征的维度

还要加上Norm，不然无法消除量纲的影响

欸，融合部分改写下，TODO
x_1 = self.attention(eeg, eye) + self.MI(eye, eeg)

x_2 = self.attention(x_2, au) + self.MI(au, x_2)

x_3 = concat(eeg, x_2)


混淆矩阵和Norm后续再考虑吧，反正现在能训起来了

# 2024-11-28 10:15，跑通了，添加Norm，优化日志，添加可视化和结果处理

一个完整的训练流程跑通了，调整batch为64，训的还蛮快

batch为64时，训练丢弃的数据还蛮多，如果效果好就不管他，如果不好就也要考虑这块儿因素

没有输出最终结果，不太好

不同人的tensorboard可视化需要加以区分

优化了日志和checkpoints的输出

config有点冗余，优化一下。已优化，适配不同模型配置和数据集配置，后续如果添加其他类型数据集或模型再修改代码。

readme不重要，上传到git以实现版本控制而已，避免编写记录丢失，2024-11-28，11：18第一次提交。

正则化处理，现有的代码跑上，把想跑的几个对比实验列一下，分析一下是否有跑的必要，设计一下自动化，然后训练一下，先跑着，在跑的同时去写下论文或者研究一下模态缺失算法等其他的。

手动提取的特征，直接去对整体做归一化存在问题，还是需要对每类特征做归一化

据我所知，眼动的话，是从3-5种眼动特征点去手动计算的41条特征，因此直接对每一种特征去做归一化，来消除量纲的影响。至于个体差异性，也要加以考虑。

人脸的话，是从17个AU强度，计算的7个特征，即119个特征，这个该怎么归一化？考虑直接些，每个特征点去做独立归一化。

Norm逻辑传参错了可还行，已修改。

写点文章，然后把实验跑上，以及列下要跑的实验种类，跑实验要巨长时间，可以先跑跨被试，这个会快些

跑个实验要两个半小时，跨被试还存在问题，调下吧